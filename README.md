# RAG Essay Evaluation
Python
Streamlit
License: MIT
Hugging Face
## DescripciÃ³n
RAG Essay Evaluation es una aplicaciÃ³n web hÃ­brida para la evaluaciÃ³n automÃ¡tica de ensayos acadÃ©micos en una escala de 1-6, utilizando tÃ©cnicas de Retrieval-Augmented Generation (RAG). Combina:

Modelo regresor fine-tuned (DistilBERT con LoRA) para puntuaciÃ³n numÃ©rica objetiva.
Vector store RAG (Supabase + embeddings de Sentence Transformers) para recuperar criterios de rÃºbricas relevantes.
LLM generativo (Gemini API) para feedback cualitativo estructurado (fortalezas, Ã¡reas de mejora, justificaciÃ³n por criterio: contenido, organizaciÃ³n, estilo, mecÃ¡nica).

Desarrollado para entornos educativos, resuelve la subjetividad en calificaciones manuales, ofreciendo anÃ¡lisis accionable. Soporta despliegue local (Streamlit) o en cloud (Hugging Face Spaces).

#### CaracterÃ­sticas Principales

Interfaz intuitiva con Streamlit: Ingresa ensayo â†’ ObtÃ©n score + feedback.
Parsing dinÃ¡mico de resultados para listas visuales.
MÃ©tricas RAGAS opcionales (context_precision, context_recall).
Modular: Backend separado para ML/RAG, frontend para UI.
Compatible con CPU; GPU opcional para aceleraciÃ³n.
AquÃ­ tienes el README en formato Markdown listo para pegar en GitHub, respetando las convenciones estÃ¡ndar, con tÃ­tulos, emojis, tablas y bloques de cÃ³digo:

```markdown
# ğŸ“š RAG Essay Evaluation System â€” EvaluaciÃ³n AutomÃ¡tica de Ensayos con RAG + LLMs + Streamlit  

Este proyecto implementa un sistema completo de **evaluaciÃ³n automÃ¡tica de ensayos acadÃ©micos** utilizando **RAG (Retrieval-Augmented Generation)**, modelos de lenguaje avanzados y una **aplicaciÃ³n web en Streamlit**.  
Incluye mÃ³dulos para recuperaciÃ³n de informaciÃ³n, inferencia, entrenamiento, evaluaciÃ³n, carga de rÃºbricas y una UI que permite evaluar ensayos en tiempo real desde el navegador.

---

## ğŸ¯ Objetivo del Proyecto

El objetivo principal es construir un pipeline integral capaz de:

- ğŸ“¥ Cargar y procesar rÃºbricas acadÃ©micas (PDF)  
- ğŸ” Recuperar informaciÃ³n relevante con RAG  
- ğŸ§  Generar evaluaciones automÃ¡ticas usando modelos LLM entrenados  
- ğŸ“ Calificar ensayos acadÃ©micos segÃºn rÃºbricas holÃ­sticas  
- ğŸ“Š Entrenar y comparar modelos  
- ğŸŒ Proveer una interfaz web en Streamlit  

Este sistema estÃ¡ diseÃ±ado para investigaciÃ³n, desarrollo acadÃ©mico y experimentaciÃ³n en evaluaciÃ³n automÃ¡tica de textos.

---

## ğŸ“‚ Estructura del Repositorio

```
rag_essay_evaluation-main/
â”œâ”€â”€ app.py               # AplicaciÃ³n Streamlit (puerto 8501)
â”œâ”€â”€ requirements.txt     # Dependencias del proyecto
â”œâ”€â”€ README.md            # Este archivo
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ config.py         # Configuraciones globales
â”‚   â”œâ”€â”€ evaluation.py     # MÃ©tricas de performance
â”‚   â”œâ”€â”€ inference.py      # Pipeline de inferencia principal
â”‚   â”œâ”€â”€ models.py         # Carga de modelos y tokenizers
â”‚   â”œâ”€â”€ pipelines.py      # Pipelines de entrenamiento e inferencia
â”‚   â”œâ”€â”€ rag.py            # Sistema RAG completo
â”‚   â”œâ”€â”€ training.py       # Fine-tuning y entrenamiento
â”‚   â””â”€â”€ utils.py          # Funciones de utilidad
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv
â”‚   â”œâ”€â”€ submission.csv
â”‚   â”œâ”€â”€ sample_submission.csv
â”‚   â”œâ”€â”€ Rubric_Holistic Essay Scoring.pdf
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tokenizer/        # Tokenizador del modelo
â”‚   â””â”€â”€ results/
â”‚       â””â”€â”€ checkpoint-XXXX/  # Modelos entrenados
â”‚
â””â”€â”€ ...
```

---

## ğŸ§  TecnologÃ­as Utilizadas

- ğŸ Python 3.10+  
- ğŸ¨ **Streamlit**  
- ğŸ¤— HuggingFace Transformers  
- ğŸ”¥ PyTorch  
- ğŸ” RAG (FAISS / embeddings)  
- ğŸ“˜ Pandas, NumPy  
- ğŸ§ª Scikit-learn  
- ğŸ“„ Procesamiento de PDF  
- ğŸ’¾ Checkpoints de modelos LLM  

---

## ğŸ“‹ Requisitos Previos

AsegÃºrate de tener instalado:

- Python 3.10+  
- pip  
- Git  
- (Opcional) GPU con CUDA  

---

## ğŸ› ï¸ InstalaciÃ³n

### 1ï¸âƒ£ Clonar el repositorio

```
git clone <URL_DE_TU_REPO>
cd rag_essay_evaluation-main
```

### 2ï¸âƒ£ Crear un entorno virtual

```
python -m venv venv
source venv/bin/activate     # Linux / Mac
venv\Scripts\activate        # Windows
```

### 3ï¸âƒ£ Instalar dependencias

```
pip install -r requirements.txt
```

---

## ğŸŒ Ejecutar la AplicaciÃ³n Streamlit

La aplicaciÃ³n principal se encuentra en:

```
app.py
```

Ejecuta:

```
streamlit run app.py
```

La aplicaciÃ³n estarÃ¡ disponible en:

```
http://localhost:8501
```

La interfaz permite:

- Cargar ensayos  
- Seleccionar o cargar rÃºbricas  
- Recuperar contexto con RAG  
- Generar puntajes automÃ¡ticos  
- Mostrar evidencia utilizada por el modelo  

---

## ğŸš€ EjecuciÃ³n Avanzada (CLI)

Ejecutar inferencia completa (RAG + modelo):

```
python app.py --essay "Tu ensayo aquÃ­" --rubric data/Rubric_Holistic Essay Scoring.pdf
```

Inferencia simple con LLM:

```
python backend/inference.py --text "Ensayo aquÃ­"
```

Usar el mÃ³dulo RAG directamente:

```
from backend.rag import RAGPipeline

rag = RAGPipeline()
context = rag.retrieve("Texto del ensayoâ€¦")
```

---

## ğŸ“ Entrenamiento del Modelo

Ejecutar el script:

```
python backend/training.py
```

Este script:

- Tokeniza el dataset  
- Entrena el modelo  
- Genera checkpoints en models/results/  

---

## ğŸ“Š EvaluaciÃ³n del Modelo

```
python backend/evaluation.py
```

MÃ©tricas generadas:

- RMSE  
- MAE  
- CorrelaciÃ³n  
- ComparaciÃ³n predicciÃ³n vs puntaje real  

---

## ğŸ§© ExplicaciÃ³n de Carpetas

- ğŸ§  backend/: Core del proyecto: RAG, modelos, entrenamiento, inferencia, evaluaciÃ³n, utilidades.  
- ğŸ“ data/: Datasets, rÃºbricas, ejemplos de submission.  
- ğŸ¤— models/: Modelos entrenados y tokenizers.  
- ğŸŒ app.py: AplicaciÃ³n completa en Streamlit.  

---

## ğŸ“Œ Notas Importantes

- La app SIEMPRE corre en el puerto 8501 (Streamlit).  
- Los checkpoints deben ir en la carpeta models/results/.  
- Las rÃºbricas deben estar en data/.  
- Si usas CPU, ajusta parÃ¡metros en backend/config.py para evitar desbordes de memoria.  

---
